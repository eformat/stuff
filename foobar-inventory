# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
etcd
nodes
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]

# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_ssh_user=root

# Debug level for all OpenShift components (defaults to 2)
debug_level=2
openshift_debug_level=2
openshift_node_debug_level=2
openshift_master_debug_level=2

# If set to true, containerized OpenShift Container Platform services are
# run on all target master and node hosts in the cluster instead of installed using RPM packages.
# If set to false or unset, the default RPM method is used.
containerized=false

# deployment type valid values are origin, online, atomic-enterprise, and openshift-enterprise
deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install, use default based on RPMs
# openshift_release=v3.6

# Install the openshift examples
openshift_install_examples=true

# Auth
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Defining htpasswd users
openshift_master_htpasswd_users={'admin':'$apr1$67sRbQHS$dLCvf5YVefoDHdiwDeM3I1'}

# Enable cockpit
osm_use_cockpit=true

# Set cockpit plugins
osm_cockpit_plugins=['cockpit-kubernetes']

# Native high availability cluster method with optional load balancer.
# If no lb group is defined, the installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=master.example.com
openshift_master_cluster_public_hostname=master.example.com

# Configure master API and console ports
openshift_master_api_port=443
openshift_master_console_port=443

# Session options
#openshift_master_session_name=ssn
#openshift_master_session_max_seconds=3600

# Global Proxy Configuration
# These options configure HTTP_PROXY, HTTPS_PROXY, and NOPROXY environment
# variables for docker and master services.
openshift_http_proxy='http://proxy.example.com:3128'
openshift_https_proxy='https://proxy.example.com:3128'
openshift_no_proxy='.example.com,localhost,127.0.0.0/8,10.50.0.0,.cluster.local'

# These options configure the BuildDefaults admission controller which injects
# environment variables into Builds. These values will default to the global proxy
# config values. You only need to set these if they differ from the global settings
# above. See BuildDefaults
# documentation at https://docs.openshift.org/latest/admin_guide/build_defaults_overrides.html
openshift_builddefaults_http_proxy='https://proxy.example.com:3128'
openshift_builddefaults_https_proxy='https://proxy.example.com:3128'
openshift_builddefaults_git_http_proxy='https://proxy.example.com:3128'
openshift_builddefaults_git_https_proxy='https://proxy.example.com:3128'
#openshift_builddefaults_no_proxy=build_defaults

# Configure usage of openshift_clock role.
#openshift_clock_enabled=true

# SDN configuration
# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#
# WARNING : Do not pick subnets that overlap with the default Docker bridge subnet of
# 172.17.0.0/16.  Your installation will fail and/or your configuration change will
# cause the Pod SDN or Cluster SDN to fail.
#
#osm_cluster_network_cidr=10.128.0.0/14
#openshift_master_external_ip_network_cidrs=['0.0.0.0/0']
#openshift_master_ingress_ip_network_cidr=172.46.0.0/16
#osm_host_subnet_length=9
#
#os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
#
openshift_use_openshift_sdn=true
os_sdn_network_plugin_name='redhat/openshift-ovs-subnet'

# Certificate Validity
#openshift_hosted_registry_cert_expire_days=730
#openshift_ca_cert_expire_days=1825
#openshift_node_cert_expire_days=730
#openshift_master_cert_expire_days=730
#etcd_ca_default_days=1825

# Router certificate (optional)
# Provide local certificate paths which will be configured as the router's default certificate.
# openshift_hosted_router_certificate={"certfile": "/path/to/router.crt", "keyfile": "/path/to/router.key", "cafile": "/path/to/router-ca.crt"}
openshift_router_selector="role=router"
openshift_hosted_router_replicas=1

# Default subdomain to use for exposed routes
osm_default_subdomain=apps.example.com
openshift_master_default_subdomain=apps.example.com

# if using xfs, you can set default max emptydir size
# openshift_node_local_quota_per_fsgroup=1024Mi

# Registry
openshift_hosted_registry_selector='registry=true'
openshift_registry_selector="role=infra"
openshift_hosted_registry_replicas=1

# Registry storage external NFS
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/registry"
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=nfs.example.com
openshift_hosted_registry_storage_nfs_directory=/mnt/stuff
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi

# Metrics
# Storage Options
# If openshift_hosted_metrics_storage_kind is unset then metrics will be stored
# in an EmptyDir volume and will be deleted when the cassandra pod terminates.
openshift_hosted_metrics_deploy=true
openshift_master_metrics_public_url=https://metrics.apps.example.com/hawkular/metrics

# Logging
# If openshift_hosted_logging_storage_kind is unset then logging will be stored
# in an EmptyDir volume and will be deleted when the elasticsearch pod terminates.
openshift_hosted_logging_deploy=true
openshift_master_logging_public_url=https://logging.apps.example.com
openshift_hosted_logging_elasticsearch_cluster_size=1

# Configure logrotate scripts
# See: https://github.com/nickhammond/ansible-logrotate
#logrotate_scripts=[{"name": "syslog", "path": "/var/log/cron\n/var/log/maillog\n/var/log/messages\n/var/log/secure\n/var/log/spooler\n", "options": ["daily", "rotate 7", "compress", "sharedscripts", "missingok"], "scripts": {"postrotate": "/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true"}}]

[masters]
ose-master03.example.com
ose-master02.example.com
ose-master01.example.com

[etcd]
ose-master03.example.com
ose-master02.example.com
ose-master01.example.com

# [lb]
ose3-lb-ansible.example.com

[nodes]
ose-master01.example.com openshift_node_labels="{'role': 'master'}" openshift_scheduleable=false
ose-master02.example.com openshift_node_labels="{'role': 'master'}" openshift_scheduleable=false
ose-master03.example.com openshift_node_labels="{'role': 'master'}" openshift_scheduleable=false
ose-infra-node01.example.com openshift_node_labels="{'role': 'infra'}"
ose-infra-node02.example.com openshift_node_labels="{'role': 'infra'}"
ose-infra-node03.example.com openshift_node_labels="{'role': 'infra', 'role': 'router'}"
ose-app-node00.example.com openshift_node_labels="{'role': 'app'}"
ose-app-node01.example.com openshift_node_labels="{'role': 'app'}"
